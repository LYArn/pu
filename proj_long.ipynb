{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Dropout, Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%cd dataset/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core PU Sequence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "topo = []\n",
    "protofold = []\n",
    "fq = []\n",
    "pu = []\n",
    "\n",
    "with open('./LISTS_CATFS_PU_PROTOTYPE_with_fused_famillies', 'r') as file:\n",
    "    for line in islice(file, 7, None):\n",
    "        tmp = line.split(maxsplit=5)\n",
    "        if len(tmp[2])>=2 and int(tmp[4])>10:\n",
    "            classes.append(tmp[0])\n",
    "            topo.append(tmp[2])\n",
    "            protofold.append(tmp[3])\n",
    "            fq.append(tmp[4])\n",
    "            pu.append(tmp[5])\n",
    "\n",
    "x = {'Class':classes, 'Topology':topo, 'Protofold':protofold, 'Frequency':fq, 'PU':pu}\n",
    "list_pu = pd.DataFrame(x, columns=['Class', 'Topology', 'Protofold', 'Frequency', 'PU'])\n",
    "list_pu = list_pu.astype({'Frequency': 'int'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort Dataframe by Frequency and\n",
    "Remove duplicate for Protofold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pu = list_pu.sort_values('Frequency', ascending=False)\n",
    "filt_pu = list_pu.drop_duplicates(subset=['Topology', 'Protofold','Frequency'], keep='first')\n",
    "filt_pu = filt_pu.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate list of pu without redundancy of pu in a protofold class (for example, 30.g219 and 50.g219 are considered the same so if a pu is present in both class, only the first one will be kept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pu = pd.DataFrame().assign(Protofold = filt_pu.iloc[:300,:]['Protofold'], PU = filt_pu.iloc[:300,:]['PU'])\n",
    "tmp_df = df_pu['PU'].str.split(' ').apply(pd.Series, 1).stack() #Split PU into rows\n",
    "tmp_df = tmp_df.replace(r'n','', regex=True).replace(r':','', regex=True) #Remove 'n' and ':'\n",
    "tmp_df.index = tmp_df.index.droplevel(-1)\n",
    "tmp_df.name = 'PU' #Need to rename to have index with which joining can be done\n",
    "del df_pu['PU']\n",
    "df_pu = df_pu.join(tmp_df)\n",
    "df_pu['Class'] = df_pu['Protofold'].str.split('.', expand = True)[1] #Get ID : 35.g129 -> g219\n",
    "df_pu = df_pu.drop_duplicates(subset=['PU', 'Class'], keep='first')\n",
    "df_pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_pu, unique_pu = [], []\n",
    "select_pu = df_pu[['Protofold', 'PU']].values.tolist()\n",
    "[unique_pu.append([proto, pu]) for proto, pu in select_pu if pu not in [item[1] for item in unique_pu]]\n",
    "print(fThere are {len(select_pu)} total PU and {len(unique_pu)} unique)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Core PU Embedding with ESMFold2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate PU for aligning and getting source sequence. Embedding will be done on unique_pu first as some of their source sequences are shared with select_pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb_gen(file):\n",
    "    with open(tmp.txt) as file:\n",
    "      f = file.read()\n",
    "      pdb = re.search(r(>>w{4}_.), f).group(0)[2:]\n",
    "      pos = re.findall(rd+-d+, f)[1]\n",
    "      yield pdb, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_result = []\n",
    "\n",
    "for proto, pu in tqdm(islice(unique_pu, len(unique_pu))):\n",
    "  name = f'{pu[0:4].lower()}_{pu[4]}'\n",
    "  !ssearch36 ./PU/$pu.fasta ./pdb_seqres.txt | grep -A2 -m1 ^>>$name > ./tmp.txt\n",
    "  \n",
    "  if os.stat(tmp.txt).st_size != 0: #If file not empty = If alignment gives result\n",
    "    for y in pdb_gen(tmp.txt):\n",
    "      pdb, pos = y\n",
    "      !grep -A1 $pdb ./pdb_seqres.txt | sed s/>{pdb}/&_{proto}_{pos}/ >> pdb_core.fasta\n",
    "  else:\n",
    "    no_result.append([proto, pu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp.fasta', 'w') as f:\n",
    "    records = SeqIO.parse('./pdb_core.fasta', 'fasta')\n",
    "    for record in records:\n",
    "        f.write(f'>{record.id}n')\n",
    "        f.write(f'{record.seq}n')\n",
    "!esm-extract esm2_t33_650M_UR50D tmp.fasta ./emb_esm2/ --include per_tok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of PU without embedding, including those in select_pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for proto, pu in unique_pu:\n",
    "    l.append([pu, proto, f'{pu[:4].lower()}_{pu[4]}_{proto}'])\n",
    "\n",
    "l_emb = []\n",
    "with open('list_emb.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        x = line.split('_')\n",
    "        l_emb.append(f'{x[0]}_{x[1]}_{x[2]}')\n",
    "\n",
    "no_emb = []\n",
    "[no_emb.append([pu, p, x]) for pu, p, x in l if x not in l_emb]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(array, target_shape):\n",
    "    return np.pad(array, [(0, target_shape[i] - array.shape[i]) for i in range(len(array.shape))], constant,)\n",
    "\n",
    "def data_gen(path_file, path_save, name, pos_start, pos_end):\n",
    "    tmp = torch.load(path_file)\n",
    "    tens = np.array(tmp[representations][33])[pos_start:pos_end]\n",
    "\n",
    "    try:\n",
    "        tens_padded = pad(tens, (60, 1280))\n",
    "    except ValueError:\n",
    "        tens_padded = np.resize(tens, (60, 1280))\n",
    "    \n",
    "    if not os.path.exists(path_save):\n",
    "        os.makedirs(path_save)\n",
    "        np.savetxt(f'{path_save}/{name}.csv', tens_padded, delimiter = ,)\n",
    "    else:\n",
    "        np.savetxt(f'{path_save}/{name}.csv', tens_padded, delimiter = ,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PU cores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list of all embedding done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = os.listdir(emb_esm2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(list_files):\n",
    "    name = f{os.path.basename(file).split('_')[0]}_{os.path.basename(file).split('_')[1]}\n",
    "    pu_class = os.path.basename(file).split('_')[2]\n",
    "    pos_start = int(file.split('_')[-1].split(-)[0])-1\n",
    "    pos_end = int(file.split('_')[-1].split('-')[-1].rsplit(.)[0])\n",
    "\n",
    "    path_save = f/home/arnaud/projet_long/dataset/core/{pu_class}\n",
    "    path_file = f/home/arnaud/projet_long/emb_esm2/{file}\n",
    "    \n",
    "    if os.path.isfile(f{path_save}/{name}.csv):\n",
    "        pass\n",
    "    else:\n",
    "        data_gen(path_file, path_save, name, pos_start, pos_end)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noncores PU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Noncore PUs\n",
    "- Directly generate padded data from embedded source sequences used for core PUs\n",
    "- If PU don't have their source sequences embedded, we generate them \n",
    "- Generate data for those PU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Generate list of noncores PU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_list_file = []\n",
    "nc_names = []\n",
    "nc_length = []\n",
    "nc_pos_start = []\n",
    "nc_pos_end = []\n",
    "with open('LIST_OF_PUs_UNCLASSED.txt', 'r') as file:\n",
    "    for line in file: #Ex : 16VPA_10_41_59.fasta\n",
    "        tmp = line.rstrip().split('_')[0]\n",
    "        nc_names.append(f{tmp[:-1].lower()}_{tmp[-1]})\n",
    "        nc_length.append(line.split('_')[1])\n",
    "        nc_pos_start.append(int(line.rstrip().split('_')[2]))\n",
    "        nc_pos_end.append(int(line.rstrip().split('_')[3].split('.')[0]))\n",
    "        nc_list_file.append(line.rstrip().split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {Filename : nc_names, 'Length' : nc_length, Start : nc_pos_start,'End' : nc_pos_end}\n",
    "df_nc = pd.DataFrame(x, columns=[Filename, Length, Start, End])\n",
    "df_nc = df_nc.astype({'Length': 'int'})\n",
    "\n",
    "df_nc = df_nc.sort_values('Length', ascending=False)\n",
    "df_modif_nc = df_nc.drop_duplicates(subset=[Filename, Length, Start, End], keep='first')\n",
    "df_modif_nc = df_modif_nc[df_modif_nc['Length'] >= 10]\n",
    "df_modif_nc = df_modif_nc.reset_index(drop=True)\n",
    "df_modif_nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_data = df_modif_nc.replace(r'_','', regex=True).astype(str).values.tolist()\n",
    "nc_data = ['_'.join(sublist).upper() for sublist in nc_data]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Generate padded data for those with source sequences embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_no_emb = []\n",
    "for filename in tqdm(nc_data):\n",
    "    name = filename.split('_')[0][:-1].lower() # To match embedding file name : 2X6WA_27_175_199 in nc_data -> 2x6w* in embedding file\n",
    "    for file in glob.glob(f'/home/arnaud/projet_long/emb_esm2/{name}*'):\n",
    "        save = /home/arnaud/projet_long/dataset/noncore/\n",
    "        nc_start = int(filename.split('_')[-2])\n",
    "        nc_end = int(filename.split('_')[-1])\n",
    "        data_gen(file, save, filename, nc_start, nc_end)\n",
    "    tmp = f{save}/{filename}.csv\n",
    "    if os.path.isfile(tmp):\n",
    "        pass\n",
    "    else:\n",
    "        nc_no_emb.append(filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) From list of PU without source sequences embedding, generate embedding with ESM-extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra = []\n",
    "tmp = [x for x in [f{items[:4].lower()}_{items[4]} for items in nc_no_emb] if x not in extra]\n",
    "extra = list(set(tmp))\n",
    "print(len(extra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp.fasta', 'w+') as f:\n",
    "    for element in tqdm(extra):    \n",
    "        records = SeqIO.parse('./pdb_seqres.txt', 'fasta')\n",
    "        for record in records:\n",
    "            if element in record.id:\n",
    "                f.write(f'>{record.id}n')\n",
    "                f.write(f'{record.seq}n')\n",
    "!esm-extract esm2_t33_650M_UR50D tmp.fasta ./emb_nc_extra/ --include per_tok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some PU don't have their source sequence in the database. In this case, we won't be using PUs associated to these sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = os.listdir(\"./emb_nc_extra/\")\n",
    "nc_emb_miss = [file for file in extra if not any(file in emb for emb in l)]\n",
    "len(nc_emb_miss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Finally, generate padded for PU left over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(nc_no_emb): #List of PU without embedding generated previously\n",
    "    name = filename.split('_')[0][:-1].lower() # To match embedding file name : 2X6WA_27_175_199 in nc_data -> 2x6w* in embedding file\n",
    "    for file in glob.glob(f'/home/arnaud/projet_long/emb_nc_extra/{name}*'):\n",
    "        save = \"/home/arnaud/projet_long/dataset/noncore/\"\n",
    "        nc_start = int(filename.split('_')[-2])\n",
    "        nc_end = int(filename.split('_')[-1])\n",
    "        data_gen(file, save, filename, nc_start, nc_end)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(split_coeff, data_list, train_data, test_data, val_data):\n",
    "    data_split = int((len(data_list))*split_coeff)\n",
    "    test_data += data_list[data_split:]\n",
    "\n",
    "    tmp_train_data = data_list[:data_split] # 80% of initial data\n",
    "    train_split = int(len(tmp_train_data)*split_coeff)\n",
    "\n",
    "    train_data += tmp_train_data[:train_split]\n",
    "    val_data += tmp_train_data[train_split:]\n",
    "    return train_data, test_data, val_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select 80% of data of each PU protofold for training and remaining 20% for test, and further split the initial 80% of training data into 80/20 for training and validation. The overall percentage of initial data will be :\n",
    "- 64% for training\n",
    "- 20% for testing\n",
    "- 16% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_count = {}\n",
    "train_data = []\n",
    "test_data = []\n",
    "val_data = []\n",
    "path = '/home/arnaud/projet_long/dataset/core_test'\n",
    "core_list = os.listdir(path)\n",
    "split_coeff = .8\n",
    "for core in core_list:\n",
    "    data_list = [f\"core/{core}/{data}\" for data in os.listdir(os.path.join(path, core))]\n",
    "    random.shuffle(data_list)\n",
    "    split_data(split_coeff, data_list, train_data, test_data, val_data)\n",
    "    data_count[core] = len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_path = '/home/arnaud/projet_long/dataset/noncore_test'\n",
    "nc_list = [f\"noncore/{data}\" for data in os.listdir(nc_path)]\n",
    "split_data(split_coeff, nc_list, train_data, test_data, val_data)\n",
    "data_count['noncore'] = len(nc_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "path = '/home/arnaud/projet_long/dataset/core'\n",
    "core_list = os.listdir(path)\n",
    "coef = int((len(core_list)+1)*.5)\n",
    "for core in core_list:\n",
    "    data_list = [f\"core/{core}/{data}\" for data in os.listdir(os.path.join(path, core))]\n",
    "    random.shuffle(data_list)\n",
    "    path_target = f'/home/arnaud/projet_long/dataset/core_test/{core}/'\n",
    "    for element in data_list[:coef]:\n",
    "        if not os.path.exists(path_target):\n",
    "            os.makedirs(path_target)\n",
    "        shutil.copy(element, path_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_target = '/home/arnaud/projet_long/dataset/noncore_test'\n",
    "nc_path = '/home/arnaud/projet_long/dataset/noncore'\n",
    "nc_list = [f\"noncore_test/{data}\" for data in os.listdir(nc_path)]\n",
    "coef = int((len(nc_list)+1)*.5)\n",
    "random.shuffle(nc_list)\n",
    "for element in nc_list[:coef]:\n",
    "    shutil.copy(element, path_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details on data generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAfter splitting data between train/test dataset,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTrain dataset contains \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_data)\u001b[39m}\u001b[39;00m\u001b[39m PUs\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTest dataset contains \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(test_data)\u001b[39m}\u001b[39;00m\u001b[39m PUs\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mVal dataset contains \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(val_data)\u001b[39m}\u001b[39;00m\u001b[39m PUs\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"After splitting data between train/test dataset,nTrain dataset contains {len(train_data)} PUsnTest dataset contains {len(test_data)} PUsnVal dataset contains {len(val_data)} PUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = len(train_data) + len(test_data) + len(val_data)\n",
    "print(f\"Total is {a} PUs\")\n",
    "print(f\"Train percentage = {round(len(train_data)*100/a)}% of total PUs\")\n",
    "print(f\"Test percentage = {round(len(test_data)*100/a)}% of total PUs\")\n",
    "print(f\"Val percentage = {round(len(val_data)*100/a)}% of total PUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu = [key for key in data_count.keys() if key != 'noncore']\n",
    "value = [value for key, value in data_count.items() if key != 'noncore']\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.bar(range(len(data_count)-1), value, tick_label = pu)\n",
    "plt.yticks(size = 10)\n",
    "plt.xlabel(\"Protofold classes\", size = 10)\n",
    "plt.ylabel(\"Number of PUs\", size = 10)\n",
    "plt.title(f\"Barplot of core PUs data, n_class = {len(data_count.keys())-1}\", size = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_key = ['Core', 'Non Core']\n",
    "binary_value = [sum(value), data_count['noncore']]\n",
    "\n",
    "plt.figure(figsize=(3, 5))\n",
    "plt.bar(binary_key, binary_value, tick_label = binary_key)\n",
    "plt.yticks(size = 8)\n",
    "plt.xlabel(\"Core PU/Non Core PU\", size = 8)\n",
    "plt.ylabel(\"Number of PUs\", size = 8)\n",
    "plt.title(f\"Barplot of core/non core PU\", size = 8)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Val, Test dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(files, batch_size, num_classes, method):\n",
    "    num_files = len(files)\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    while True:\n",
    "        np.random.shuffle(files)\n",
    "        for i in range(0, num_files, batch_size):\n",
    "            batch_files = files[i:i+batch_size]\n",
    "            batch_data = []\n",
    "            batch_labels = []\n",
    "            for file in batch_files:\n",
    "                data = pd.read_csv(file, header=None).values\n",
    "                batch_data.append(data)\n",
    "                label = tf.strings.split(file, os.path.sep)[-2]\n",
    "                if method == \"binary\":\n",
    "                    if label == 'noncore':\n",
    "                        batch_labels.append(0)\n",
    "                    else:\n",
    "                        batch_labels.append(1)\n",
    "                else:\n",
    "                    batch_labels.append(label)\n",
    "            \n",
    "            batch_data = np.array(batch_data)\n",
    "            if method == \"binary\":\n",
    "                if label == 'noncore':\n",
    "                    batch_labels = np.array(batch_labels)\n",
    "                    batch_labels = to_categorical(batch_labels, 2)\n",
    "                else:\n",
    "                    batch_labels = label_encoder.fit_transform(batch_labels)  # Encode labels as integers\n",
    "                    batch_labels = to_categorical(batch_labels, num_classes)\n",
    "            \n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data : \n",
    "- data_count.keys() containing all protofold classes\n",
    "- train_data and test_data containing list of files for the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = len(data_count.keys())\n",
    "\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(val_data)\n",
    "random.shuffle(test_data)\n",
    "train_generator = data_generator(train_data, batch_size, num_classes, method = \"binary\")\n",
    "val_generator = data_generator(val_data, batch_size, num_classes, method = \"binary\")\n",
    "test_generator = data_generator(test_data, batch_size, num_classes, method = \"binary\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN (differenciation between PU/not PU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn():\n",
    "  inputs = Input(shape=(60, 1280, 1))\n",
    "  conv_1 = Conv2D(64,(6,9), activation = \"relu\")(inputs)\n",
    "  pool_1 = MaxPooling2D(pool_size = (3,3))(conv_1)\n",
    "  conv_2 = Conv2D(32,(6,9), activation = \"relu\")(pool_1)\n",
    "  pool_2 = MaxPooling2D(pool_size = (3,3))(conv_2)\n",
    "  flat = Flatten()(pool_2)\n",
    "  dense_1 = Dense(activation = 'relu', units = 128)(flat)\n",
    "  drop = Dropout(0.2)(dense_1)\n",
    "  dense_2 = Dense(activation = 'softmax', units = 1)(drop)\n",
    "  model = Model(inputs=inputs, outputs=dense_2)\n",
    "  return model\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min',\n",
    "    verbose=1)\n",
    "model = cnn()\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_data) // batch_size\n",
    "cnn_model = model.fit(train_generator, validation_data=val_generator, steps_per_epoch = steps_per_epoch, epochs=25, callbacks=[early_stopping_callback])\n",
    "model.save('./cnn_save/model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet_long",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
